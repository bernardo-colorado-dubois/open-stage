# Open-Stage - Complete Knowledge Base
## Version 2.4 - January 2025

---

# PROJECT OVERVIEW

**Open-Stage** is an enterprise-grade ETL (Extract, Transform, Load) platform built in Python, inspired by IBM DataStage. It implements a pipes and filters architecture that enables the creation of modular, scalable data processing pipelines with multi-model generative AI capabilities.

## Key Statistics
- **Total Components**: 29 (5 base + 24 specialized)
- **License**: MIT
- **Authors**: Bernardo Colorado Dubois, Saul Hernandez Cordova
- **Python Version**: 3.8+
- **Current Version**: 2.4
- **Code Style**: 2-space indentation

## Core Features
- 29 Modular Components (5 base + 24 specialized)
- Multiple Data Sources: CSV, MySQL, PostgreSQL, BigQuery, REST APIs, In-Memory DataFrames
- AI-Powered Transformations: OpenAI (GPT-4o, GPT-4-Turbo), Claude (Anthropic), Gemini (Google), DeepSeek
- Robust Validations and intelligent error handling
- Method Chaining for fluent syntax
- Extensible Architecture by provider and component type
- **Advanced BigQuery Support**: before_query, after_query, partitioning, clustering, dry_run
- **Advanced PostgreSQL Support**: before_query, after_query, timeout, query_parameters ‚ú®
- **Advanced MySQL Support**: before_query, after_query, timeout, query_parameters ‚ú® NEW v2.4

---

# PROJECT STRUCTURE
```
project/
‚îú‚îÄ‚îÄ LICENSE                    # MIT License
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py                    # Base classes (5)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ DataPackage           # Encapsulates data + metadata
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Pipe                  # Connects components
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Origin                # Abstract class for sources (0‚Üí1)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Destination           # Abstract class for sinks (1‚Üí0)
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Node                  # Abstract class for transformers
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ common.py                  # Generic components (15)
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ Generator             # Origin: Sequential data
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ CSVOrigin             # Origin: CSV files
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ APIRestOrigin         # Origin: REST APIs
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ OpenOrigin            # Origin: In-memory DataFrames
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ Printer               # Destination: Console output
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ CSVDestination        # Destination: CSV files
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ Funnel                # Node/Router: N‚Üí1 combine
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ Switcher              # Node/Router: 1‚ÜíN conditional
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ Copy                  # Node/Router: 1‚ÜíN duplicate
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ Filter                # Node/Transformer: Row filtering (9 ops)
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ Aggregator            # Node/Transformer: Aggregations
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ DeleteColumns         # Node/Transformer: Remove columns
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ RemoveDuplicates      # Node/Transformer: Deduplication
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ Joiner                # Node/Transformer: Joins (2‚Üí1)
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ Transformer           # Node/Transformer: Custom functions
‚îÇ   ‚îú‚îÄ‚îÄ postgres/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ common.py                  # PostgreSQL connectors (2) ‚ú® ENHANCED v2.4
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ PostgresOrigin        # Origin: PostgreSQL queries
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ PostgresDestination   # Destination: PostgreSQL tables
‚îÇ   ‚îú‚îÄ‚îÄ mysql/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ common.py                  # MySQL connectors (2) ‚ú® ENHANCED v2.4
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ MySQLOrigin           # Origin: MySQL queries
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ MySQLDestination      # Destination: MySQL tables
‚îÇ   ‚îú‚îÄ‚îÄ google/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cloud.py                   # GCP BigQuery services (2)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ GCPBigQueryOrigin     # Origin: BigQuery queries ‚ú® ENHANCED
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ GCPBigQueryDestination # Destination: BigQuery load ‚ú® ENHANCED
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ gemini.py                  # Google AI (1)
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ GeminiPromptTransformer # Node: Gemini transformations
‚îÇ   ‚îú‚îÄ‚îÄ anthropic/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ claude.py                  # Anthropic AI (1)
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ AnthropicPromptTransformer # Node: Claude transformations
‚îÇ   ‚îú‚îÄ‚îÄ deepseek/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ deepseek.py                # DeepSeek AI (1)
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ DeepSeekPromptTransformer # Node: DeepSeek transformations
‚îÇ   ‚îî‚îÄ‚îÄ open_ai/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ chat_gpt.py                # OpenAI AI (1)
‚îÇ           ‚îî‚îÄ‚îÄ OpenAIPromptTransformer # Node: GPT transformations
```

---

# COMPONENTS CATALOG

## Origins (Data Sources) - 0‚Üí1

### MySQLOrigin ‚ú® ENHANCED v2.4
**Module**: `src/mysql/common.py`
**Purpose**: Queries MySQL databases with advanced features.

**Dependencies**: `sqlalchemy`, `pymysql`

**Parameters**:
- `name`: str - Component name
- `host`: str - MySQL host
- `port`: int = 3306 - MySQL port
- `database`: str - Database name
- `user`: str - Username
- `password`: str - Password
- `query`: str (optional) - SQL query to execute (required if table not provided)
- `table`: str (optional) - Table reference in format 'table' or 'database.table' (required if query not provided)
- **`before_query`: str = None** - ‚ú® SQL query to execute BEFORE extraction
- **`after_query`: str = None** - ‚ú® SQL query to execute AFTER extraction
- **`max_results`: int = None** - ‚ú® Maximum rows to return (adds LIMIT)
- **`timeout`: float = None** - ‚ú® Query timeout in seconds
- **`query_parameters`: dict = None** - ‚ú® Dictionary of query parameters for parameterized queries

**Connection String**: `mysql+pymysql://user:pass@host:port/database`

**Example**:
```python
mysql = MySQLOrigin(
  name="mysql_reader",
  host="localhost",
  database="company_db",
  user="root",
  password="password",
  query="SELECT * FROM customers WHERE active = 1"
)
```

**New Features v2.4**:
- **before_query**: Create temp tables, call procedures, prepare data before extraction
- **after_query**: Audit logging, cleanup, post-processing after extraction
- **table**: Direct table read without writing SELECT *
- **max_results**: Limit rows for testing without modifying query
- **timeout**: Control query execution time
- **query_parameters**: Secure parameterized queries using :param_name syntax
- **Enhanced logging**: Duration, rows returned, data types, detailed statistics

**Example with advanced features**:
```python
mysql = MySQLOrigin(
  name="daily_sales",
  host="localhost",
  database="warehouse",
  user="root",
  password="password",
  before_query="""
    CREATE TEMPORARY TABLE staging_sales AS
    SELECT * FROM raw_sales WHERE date = CURDATE();
  """,
  query="SELECT * FROM staging_sales WHERE amount > :min_amount",
  query_parameters={'min_amount': 100.0},
  max_results=1000,
  timeout=300,
  after_query="""
    INSERT INTO audit.extraction_log (table_name, extracted_at)
    VALUES ('staging_sales', NOW());
  """
)
```

**Validations**:
- host, database, user, password cannot be empty
- Must specify either query OR table (not both)
- port must be positive
- max_results must be positive if specified
- timeout must be positive if specified
- before_query and after_query cannot be empty strings if specified

**Error Handling**:
- Connection refused ‚Üí Check if MySQL is running
- Authentication failed ‚Üí Check credentials
- Database not found ‚Üí Check if database exists
- SQL syntax errors ‚Üí Shows problematic query
- Timeout errors ‚Üí Shows configured timeout value

---

### PostgresOrigin ‚ú® ENHANCED v2.4
**Module**: `src/postgres/common.py`
**Purpose**: Queries PostgreSQL databases with advanced features.

**Dependencies**: `sqlalchemy`, `psycopg2-binary`

**Parameters**:
- `name`: str - Component name
- `host`: str - PostgreSQL host
- `port`: int = 5432 - PostgreSQL port
- `database`: str - Database name
- `user`: str - Username
- `password`: str - Password
- `query`: str (optional) - SQL query to execute (required if table not provided)
- `table`: str (optional) - Table reference in format 'table' or 'schema.table' (required if query not provided)
- **`before_query`: str = None** - ‚ú® SQL query to execute BEFORE extraction
- **`after_query`: str = None** - ‚ú® SQL query to execute AFTER extraction
- **`max_results`: int = None** - ‚ú® Maximum rows to return (adds LIMIT)
- **`timeout`: float = None** - ‚ú® Query timeout in seconds
- **`query_parameters`: dict = None** - ‚ú® Dictionary of query parameters for parameterized queries

**Connection String**: `postgresql+psycopg2://user:pass@host:port/database`

**Example**:
```python
pg = PostgresOrigin(
  name="pg_reader",
  host="localhost",
  database="analytics_db",
  user="postgres",
  password="password",
  query="SELECT * FROM sales WHERE date >= '2024-01-01'"
)
```

---

### GCPBigQueryOrigin ‚ú® ENHANCED
**Module**: `src/google/cloud.py`
**Purpose**: Queries Google BigQuery with advanced features.

**Dependencies**: `google-cloud-bigquery`, `google-auth`, `db-dtypes`

**Parameters**:
- `name`: str - Component name
- `project_id`: str - GCP project ID
- `query`: str (optional) - BigQuery SQL query (required if table not provided)
- `table`: str (optional) - Table reference in format 'dataset.table' or 'project.dataset.table'
- `credentials_path`: str = None - Path to service account JSON (optional)
- **`before_query`: str = None** - ‚ú® SQL query to execute BEFORE extraction
- **`after_query`: str = None** - ‚ú® SQL query to execute AFTER extraction
- **`max_results`: int = None** - ‚ú® Maximum rows to return (for testing)
- **`use_legacy_sql`: bool = False** - ‚ú® Use legacy SQL syntax
- **`query_parameters`: list = None** - ‚ú® Query parameters for parameterized queries
- **`location`: str = None** - ‚ú® BigQuery location (e.g., 'US', 'EU')
- **`job_labels`: dict = {}** - ‚ú® Labels for the BigQuery job
- **`timeout`: float = None** - ‚ú® Query timeout in seconds
- **`use_query_cache`: bool = True** - ‚ú® Use cached results if available
- **`dry_run`: bool = False** - ‚ú® Validate query and estimate cost without executing

---

## Destinations (Data Sinks) - 1‚Üí0

### MySQLDestination ‚ú® ENHANCED v2.4
**Module**: `src/mysql/common.py`
**Purpose**: Writes data to MySQL tables with advanced features.

**Dependencies**: `sqlalchemy`, `pymysql`

**Parameters**:
- `name`: str - Component name
- `host`: str - MySQL host
- `port`: int = 3306 - MySQL port
- `database`: str - Database name
- `user`: str - Username
- `password`: str - Password
- `table`: str - Table name
- `if_exists`: str = 'append' - Behavior if table exists
- **`before_query`: str = None** - ‚ú® SQL query to execute BEFORE loading data
- **`after_query`: str = None** - ‚ú® SQL query to execute AFTER loading data
- **`timeout`: float = None** - ‚ú® Query timeout in seconds

**if_exists options**:
- `'fail'` - Error if table exists
- `'replace'` - Drop and recreate table
- `'append'` - Add rows to existing table

**Example**:
```python
mysql_dest = MySQLDestination(
  name="mysql_writer",
  host="localhost",
  database="dulceria",
  user="root",
  password="password",
  table="compras_importantes",
  if_exists="append"
)
```

**New Features v2.4**:
- **before_query**: Create backups, truncate tables, prepare staging before load
- **after_query**: Audit logging, data validation, update statistics after load
- **timeout**: Control connection and query execution time
- **Enhanced logging**: Rows loaded, duration, data types, detailed statistics

**Example with advanced features**:
```python
mysql_dest = MySQLDestination(
  name="sales_loader",
  host="localhost",
  database="warehouse",
  user="root",
  password="password",
  table="sales_fact",
  before_query="""
    -- Create backup
    DROP TABLE IF EXISTS sales_fact_backup;
    CREATE TABLE sales_fact_backup AS SELECT * FROM sales_fact;
    
    -- Disable foreign key checks
    SET FOREIGN_KEY_CHECKS = 0;
  """,
  if_exists="replace",
  timeout=600,
  after_query="""
    -- Re-enable foreign key checks
    SET FOREIGN_KEY_CHECKS = 1;
    
    -- Update statistics
    ANALYZE TABLE sales_fact;
    
    -- Log the load
    INSERT INTO audit.load_log (table_name, loaded_at, record_count)
    VALUES ('sales_fact', NOW(), (SELECT COUNT(*) FROM sales_fact));
  """
)
```

**Optimizations**:
- Writes in chunks of 1000 rows
- No index included (index=False)

**Validations**:
- host, database, user, password, table cannot be empty
- port must be positive
- if_exists must be one of: 'fail', 'replace', 'append'
- before_query and after_query cannot be empty strings if specified
- timeout must be positive if specified

**Error Handling**:
- Connection refused ‚Üí Check if MySQL is running
- Authentication failed ‚Üí Check credentials
- Database not found ‚Üí Check if database exists
- Table already exists ‚Üí Use if_exists='replace' or 'append'
- Timeout errors ‚Üí Shows configured timeout value

---

### PostgresDestination ‚ú® ENHANCED v2.4
**Module**: `src/postgres/common.py`
**Purpose**: Writes data to PostgreSQL tables with advanced features.

**Dependencies**: `sqlalchemy`, `psycopg2-binary`

**Parameters**:
- `name`: str - Component name
- `host`: str - PostgreSQL host
- `port`: int = 5432 - PostgreSQL port
- `database`: str - Database name
- `user`: str - Username
- `password`: str - Password
- `table`: str - Table name
- `schema`: str = 'public' - PostgreSQL schema
- `if_exists`: str = 'append' - Behavior if table exists
- **`before_query`: str = None** - ‚ú® SQL query to execute BEFORE loading data
- **`after_query`: str = None** - ‚ú® SQL query to execute AFTER loading data
- **`timeout`: float = None** - ‚ú® Query timeout in seconds

---

### GCPBigQueryDestination ‚ú® ENHANCED
**Module**: `src/google/cloud.py`
**Purpose**: Loads data to Google BigQuery with advanced features.

**Dependencies**: `google-cloud-bigquery`, `google-auth`

**Parameters**:
- `name`: str - Component name
- `project_id`: str - GCP project ID
- `dataset`: str - BigQuery dataset
- `table`: str - Table name
- `write_disposition`: str - Write mode
- `credentials_path`: str = None - Path to service account JSON (optional)
- **`before_query`: str = None** - ‚ú® SQL query to execute BEFORE loading
- **`after_query`: str = None** - ‚ú® SQL query to execute AFTER loading
- **`schema`: list = None** - ‚ú® BigQuery schema (auto-detected if None)
- **`create_disposition`: str = 'CREATE_IF_NEEDED'** - ‚ú® Table creation behavior
- **`schema_update_options`: list = []** - ‚ú® Schema update options
- **`clustering_fields`: list = None** - ‚ú® Fields for clustering (max 4)
- **`time_partitioning`: dict = None** - ‚ú® Time partitioning configuration
- **`location`: str = None** - ‚ú® BigQuery location
- **`job_labels`: dict = {}** - ‚ú® Labels for the BigQuery job
- **`max_bad_records`: int = 0** - ‚ú® Maximum bad records to tolerate
- **`autodetect`: bool = True** - ‚ú® Auto-detect schema from DataFrame

---

# DESIGN PRINCIPLES

1. **Pipes and Filters Architecture**: Modular, reusable components with clear separation of concerns
2. **Robust Validations**: Validation at construction time, data type validation, connectivity validation
3. **Error Handling**: Try-catch in critical operations, detailed error logging, graceful recovery
4. **Default Connectivity**: Origin (0‚Üí1), Destination (1‚Üí0), Node (flexible via override)
5. **Method Chaining**: Fluent syntax via `set_destination()` returning Node
6. **Lazy Initialization**: External clients initialized on demand
7. **Immediate Processing**: `sink()` automatically calls `pump()`
8. **Resource Cleanup**: DataFrames cleaned post-processing, connections closed appropriately
9. **2-Space Indentation**: Consistent code style throughout the project
10. **Enhanced Logging**: Structured logging with emojis (‚úÖ ‚ùå üìä ‚è±Ô∏è üìã) for better readability ‚ú®

---

# EXAMPLE PATTERNS

## Pattern 1: Simple ETL
```python
origin = CSVOrigin("reader", filepath_or_buffer="data.csv")
transform = Filter("filter", "age", ">=", 18)
destination = CSVDestination("writer", path_or_buf="output.csv", index=False)

pipe1, pipe2 = Pipe("p1"), Pipe("p2")

origin.add_output_pipe(pipe1).set_destination(transform)
transform.add_output_pipe(pipe2).set_destination(destination)

origin.pump()
```

## Pattern 2: MySQL with Advanced Features ‚ú® NEW v2.4
```python
# Extract with before_query
mysql_origin = MySQLOrigin(
  name="sales_extract",
  host="localhost",
  database="warehouse",
  user="root",
  password="password",
  before_query="""
    CREATE TEMPORARY TABLE staging AS
    SELECT * FROM raw_sales WHERE date = CURDATE();
  """,
  query="SELECT * FROM staging WHERE amount > :min_amount",
  query_parameters={'min_amount': 100.0},
  max_results=10000,
  timeout=300,
  after_query="""
    INSERT INTO audit.extraction_log (table_name, extracted_at)
    VALUES ('staging', NOW());
  """
)

# Transform
filter_node = Filter("high_value", "amount", ">", 1000)

# Load with before/after queries
mysql_dest = MySQLDestination(
  name="sales_load",
  host="localhost",
  database="warehouse",
  user="root",
  password="password",
  table="sales_fact",
  before_query="""
    -- Create backup
    DROP TABLE IF EXISTS sales_fact_backup;
    CREATE TABLE sales_fact_backup AS SELECT * FROM sales_fact;
    -- Disable foreign key checks
    SET FOREIGN_KEY_CHECKS = 0;
  """,
  if_exists="replace",
  timeout=600,
  after_query="""
    -- Re-enable foreign key checks
    SET FOREIGN_KEY_CHECKS = 1;
    -- Update statistics
    ANALYZE TABLE sales_fact;
    -- Log load
    INSERT INTO audit.load_log (table_name, loaded_at)
    VALUES ('sales_fact', NOW());
  """
)

# Connect
pipe1, pipe2 = Pipe("extract"), Pipe("load")
mysql_origin.add_output_pipe(pipe1).set_destination(filter_node)
filter_node.add_output_pipe(pipe2).set_destination(mysql_dest)

# Execute
mysql_origin.pump()
```

## Pattern 3: PostgreSQL with Advanced Features ‚ú® v2.4
```python
# Extract with before_query
pg_origin = PostgresOrigin(
  name="sales_extract",
  host="localhost",
  database="warehouse",
  user="postgres",
  password="password",
  before_query="""
    CREATE TEMP TABLE staging AS
    SELECT * FROM raw_sales WHERE date = CURRENT_DATE;
  """,
  query="SELECT * FROM staging WHERE amount > :min_amount",
  query_parameters={'min_amount': 100.0},
  max_results=10000,
  timeout=300,
  after_query="""
    INSERT INTO audit.extraction_log (table_name, extracted_at)
    VALUES ('staging', NOW());
  """
)

# Transform
filter_node = Filter("high_value", "amount", ">", 1000)

# Load with before/after queries
pg_dest = PostgresDestination(
  name="sales_load",
  host="localhost",
  database="warehouse",
  user="postgres",
  password="password",
  table="sales_fact",
  schema="public",
  before_query="""
    -- Create backup
    CREATE TABLE sales_fact_backup AS SELECT * FROM sales_fact;
    -- Disable triggers
    ALTER TABLE sales_fact DISABLE TRIGGER ALL;
  """,
  if_exists="replace",
  timeout=600,
  after_query="""
    -- Re-enable triggers
    ALTER TABLE sales_fact ENABLE TRIGGER ALL;
    -- Refresh views
    REFRESH MATERIALIZED VIEW reports.sales_summary;
    -- Log load
    INSERT INTO audit.load_log (table_name, loaded_at)
    VALUES ('sales_fact', NOW());
    -- Update statistics
    ANALYZE sales_fact;
  """
)

# Connect
pipe1, pipe2 = Pipe("extract"), Pipe("load")
pg_origin.add_output_pipe(pipe1).set_destination(filter_node)
filter_node.add_output_pipe(pipe2).set_destination(pg_dest)

# Execute
pg_origin.pump()
```

## Pattern 4: BigQuery with Advanced Features ‚ú® EXISTING
```python
# Extract with before_query and dry_run
bq_origin = GCPBigQueryOrigin(
  name="sales_extract",
  project_id="my-project",
  before_query="""
    CREATE TEMP TABLE staging AS
    SELECT * FROM raw_sales WHERE date = CURRENT_DATE()
  """,
  query="SELECT * FROM staging",
  dry_run=False,
  max_results=10000
)

# Transform
filter_node = Filter("high_value", "amount", ">", 1000)

# Load with partitioning and after_query
bq_dest = GCPBigQueryDestination(
  name="sales_load",
  project_id="my-project",
  dataset="warehouse",
  table="sales",
  write_disposition="WRITE_APPEND",
  time_partitioning={'type': 'DAY', 'field': 'sale_date'},
  clustering_fields=['region', 'category'],
  after_query="""
    INSERT INTO audit.load_log (table_name, loaded_at, record_count)
    VALUES ('sales', CURRENT_TIMESTAMP(), (SELECT COUNT(*) FROM warehouse.sales))
  """
)

# Connect
pipe1, pipe2 = Pipe("extract"), Pipe("load")
bq_origin.add_output_pipe(pipe1).set_destination(filter_node)
filter_node.add_output_pipe(pipe2).set_destination(bq_dest)

# Execute
bq_origin.pump()
```

---

# ROADMAP

## Completed (‚úÖ)
- MySQL Origin and Destination ‚ú® **ENHANCED v2.4**
- PostgreSQL Origin and Destination ‚ú® **ENHANCED v2.4**
- BigQuery Origin and Destination with advanced features ‚ú®
- OpenOrigin for in-memory DataFrames
- OpenAI Transformer (GPT-4o, GPT-4-Turbo, GPT-3.5-Turbo)
- Anthropic Transformer (Claude)
- Google Transformer (Gemini)
- DeepSeek Transformer
- **GCPBigQueryOrigin enhancements**: before_query, after_query, dry_run, max_results ‚ú®
- **GCPBigQueryDestination enhancements**: before_query, after_query, partitioning, clustering ‚ú®
- **PostgresOrigin enhancements**: before_query, after_query, timeout, max_results, query_parameters, table ‚ú®
- **PostgresDestination enhancements**: before_query, after_query, timeout ‚ú®
- **MySQLOrigin enhancements**: before_query, after_query, timeout, max_results, query_parameters, table ‚ú® **NEW v2.4**
- **MySQLDestination enhancements**: before_query, after_query, timeout ‚ú® **NEW v2.4**
- **29 total components**

## In Progress (üöß)
- [ ] Documentation improvements (expanding guides for remaining components)

## Pending AI Providers
- [ ] Mistral AI Transformer
- [ ] Cohere Transformer
- [ ] Llama Transformer (via Ollama/local)
- [ ] Azure OpenAI Transformer

## Potential Components

### Origins
- [ ] MariaDB, MongoDB, Kafka Consumer
- [ ] S3 (AWS), Azure Blob Storage, Snowflake
- [ ] Excel, Parquet, JSON, XML, SFTP

### Destinations
- [ ] MariaDB, MongoDB, Kafka Producer
- [ ] S3 (AWS), Azure Blob Storage, Snowflake
- [ ] Excel, Parquet, JSON, XML, SFTP

### Transformers
- [ ] Sort, Pivot, Unpivot, Window Functions
- [ ] Lookup, Merge, Split, Sample
- [ ] Normalize, Encode

---

# NOTES FOR FUTURE DEVELOPMENT

## When creating new Origins:
1. Inherit from `Origin` class
2. Use default 0‚Üí1 connectivity (no override needed for simple cases)
3. Initialize external clients in `_initialize_client()` (lazy initialization)
4. Implement `pump()` method
5. Add comprehensive error handling
6. Close connections in `finally` block
7. Use 2-space indentation
8. **Consider adding `before_query` and `after_query` parameters for advanced workflows** ‚ú®
9. **Consider adding `timeout`, `max_results`, and `query_parameters` for flexibility** ‚ú®
10. **Use structured logging with separators and emojis** ‚ú®
11. **Implement `_execute_query()` method for before/after query execution** ‚ú®
12. **Implement `_build_query()` method if supporting both query and table parameters** ‚ú®

## When creating new Destinations:
1. Inherit from `Destination` class
2. Use default 1‚Üí0 connectivity (no override needed for simple cases)
3. Initialize external clients in `_initialize_client()` (lazy initialization)
4. Implement `sink()` method
5. Add comprehensive error handling
6. Close connections in `finally` block
7. Use 2-space indentation
8. **Consider adding `before_query` and `after_query` parameters for advanced workflows** ‚ú®
9. **Consider adding `timeout` parameter for control** ‚ú®
10. **Consider performance optimizations like partitioning and clustering** ‚ú®
11. **Use structured logging with separators and emojis** ‚ú®
12. **Implement `_execute_query()` method for before/after query execution** ‚ú®

## Database Enhancement Pattern (v2.4) ‚ú®
The PostgreSQL and MySQL components serve as reference patterns for enhancing database connectors:

**Origin enhancements**:
- `before_query`: Create temp tables, call functions, set session variables
- `after_query`: Audit logging, mark records as processed, cleanup
- `table`: Simplified table read (supports 'table' or 'schema.table' / 'database.table')
- `max_results`: Add LIMIT automatically for testing
- `timeout`: Control connection and query execution time
- `query_parameters`: Parameterized queries using :param_name syntax (dict)

**Destination enhancements**:
- `before_query`: Create backups, truncate tables, disable triggers/foreign keys
- `after_query`: Validate data, refresh views, re-enable triggers, ANALYZE/OPTIMIZE
- `timeout`: Control connection and query execution time

**Logging improvements**:
- Structured format with `{'='*70}` separators
- Emojis: ‚úÖ ‚ùå üìä ‚è±Ô∏è üìã for visual clarity
- Duration tracking with `time.time()`
- Detailed statistics (rows, columns, data types)
- Separate sections for before/main/after queries

**Error handling**:
- Specific error messages for common failures
- Timeout detection and reporting
- Query preview in error messages

**MySQL vs PostgreSQL differences**:
- MySQL uses backticks for identifiers: \`table\`
- PostgreSQL uses double quotes: "table"
- MySQL: `TEMPORARY TABLE`, PostgreSQL: `TEMP TABLE`
- MySQL: `CURDATE()` / `NOW()`, PostgreSQL: `CURRENT_DATE` / `NOW()`
- MySQL: `ANALYZE TABLE`, PostgreSQL: `ANALYZE table`
- MySQL: `OPTIMIZE TABLE`, PostgreSQL: `VACUUM`
- MySQL: `SET FOREIGN_KEY_CHECKS = 0`, PostgreSQL: `ALTER TABLE DISABLE TRIGGER ALL`

This pattern should be applied to MariaDB and other database connectors.

---

**Version**: 2.4  
**Date**: January 2025  
**Status**: Production Ready ‚úÖ  
**Latest Updates**: 
- Enhanced MySQLOrigin with before_query, after_query, timeout, max_results, query_parameters, table ‚ú® **NEW**
- Enhanced MySQLDestination with before_query, after_query, timeout ‚ú® **NEW**
- Enhanced PostgresOrigin with before_query, after_query, timeout, max_results, query_parameters, table ‚ú®
- Enhanced PostgresDestination with before_query, after_query, timeout ‚ú®
- Enhanced logging with structured format and emojis ‚ú®
- Added database enhancement pattern for reference ‚ú®

---

**END OF KNOWLEDGE BASE**