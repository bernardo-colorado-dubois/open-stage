# GCPBigQueryOrigin - Gu√≠a de Uso

Componente para extraer datos de Google BigQuery con capacidades avanzadas.

---

## üéØ Caracter√≠sticas

- ‚úÖ Lectura directa de tablas o queries personalizadas
- ‚úÖ Queries pre y post extracci√≥n (`before_query`, `after_query`)
- ‚úÖ L√≠mite de resultados para testing (`max_results`)
- ‚úÖ Validaci√≥n de queries sin ejecutar (`dry_run`)
- ‚úÖ Queries parametrizadas
- ‚úÖ Estimaci√≥n de costos autom√°tica
- ‚úÖ Logging detallado

---

## üì¶ Instalaci√≥n
```bash
pip install google-cloud-bigquery google-auth db-dtypes
```

---

## üöÄ Uso B√°sico

### Ejemplo 1: Query Simple
```python
from src.google.cloud import GCPBigQueryOrigin
from src.core.base import Pipe
from src.core.common import Printer

# Crear origen con query
origin = GCPBigQueryOrigin(
    name="sales_data",
    project_id="my-project",
    query="SELECT * FROM dataset.sales WHERE date >= '2024-01-01'"
)

# Conectar y ejecutar
pipe = Pipe("pipe1")
printer = Printer("output")

origin.add_output_pipe(pipe).set_destination(printer)
origin.pump()
```

---

### Ejemplo 2: Lectura Directa de Tabla
```python
# Leer tabla completa sin escribir SELECT *
origin = GCPBigQueryOrigin(
    name="customers",
    project_id="my-project",
    table="dataset.customers"  # ‚ú® M√°s simple!
)

origin.add_output_pipe(pipe).set_destination(printer)
origin.pump()
```

---

### Ejemplo 3: L√≠mite para Testing
```python
# Solo extraer 100 filas para pruebas
origin = GCPBigQueryOrigin(
    name="sales_sample",
    project_id="my-project",
    table="dataset.sales",
    max_results=100  # ‚ú® R√°pido para desarrollo
)

origin.add_output_pipe(pipe).set_destination(printer)
origin.pump()
```

---

## üîß Funcionalidades Avanzadas

### Ejemplo 4: Dry Run (Validar y Estimar Costos)
```python
# Validar query SIN ejecutar
origin = GCPBigQueryOrigin(
    name="cost_check",
    project_id="my-project",
    query="SELECT * FROM `bigquery-public-data.usa_names.usa_1910_current`",
    dry_run=True  # ‚ú® Solo valida y estima costo
)

origin.add_output_pipe(pipe).set_destination(printer)
origin.pump()

# Output:
# ‚úÖ Query is valid
# üìä Estimated bytes processed: 6,432,432 bytes (6.13 MB)
# üí∞ Estimated cost: $0.000038 USD
```

---

### Ejemplo 5: Before Query (Preparar Datos)
```python
# Ejecutar query ANTES de la extracci√≥n
origin = GCPBigQueryOrigin(
    name="processed_sales",
    project_id="my-project",
    before_query="""
        -- Crear tabla temporal con datos filtrados
        CREATE TEMP TABLE temp_sales AS
        SELECT * FROM dataset.raw_sales
        WHERE status = 'completed'
        AND date >= '2024-01-01';
    """,
    query="SELECT * FROM temp_sales WHERE amount > 100"
)

origin.add_output_pipe(pipe).set_destination(printer)
origin.pump()
```

**Casos de uso de `before_query`:**
- Crear tablas temporales
- Llamar stored procedures
- Preparar datos antes de extraer
- Limpiar staging areas

---

### Ejemplo 6: After Query (Auditor√≠a)
```python
# Ejecutar query DESPU√âS de la extracci√≥n
origin = GCPBigQueryOrigin(
    name="customer_extract",
    project_id="my-project",
    table="crm.customers",
    after_query="""
        -- Registrar la extracci√≥n
        INSERT INTO `my-project.audit.extraction_log` (
            table_name,
            extracted_at,
            record_count
        ) VALUES (
            'customers',
            CURRENT_TIMESTAMP(),
            (SELECT COUNT(*) FROM `my-project.crm.customers`)
        );
    """
)

origin.add_output_pipe(pipe).set_destination(printer)
origin.pump()
```

**Casos de uso de `after_query`:**
- Logging de auditor√≠a
- Marcar registros como procesados
- Actualizar timestamps
- Limpiar tablas temporales

---

### Ejemplo 7: Workflow Completo (Before + After)
```python
# Pipeline completo con preparaci√≥n y limpieza
origin = GCPBigQueryOrigin(
    name="daily_sales_etl",
    project_id="my-project",
    
    # ANTES: Preparar staging
    before_query="""
        -- Crear tabla staging
        CREATE OR REPLACE TABLE `my-project.staging.daily_sales` AS
        SELECT 
            DATE(order_timestamp) as sale_date,
            product_id,
            customer_id,
            amount,
            region
        FROM `my-project.raw.orders`
        WHERE DATE(order_timestamp) = CURRENT_DATE()
        AND status = 'completed';
        
        -- Validar datos
        CALL `my-project.procedures.validate_sales`();
    """,
    
    # QUERY PRINCIPAL
    query="SELECT * FROM `my-project.staging.daily_sales`",
    
    # DESPU√âS: Registrar y limpiar
    after_query="""
        -- Registrar ejecuci√≥n
        INSERT INTO `my-project.audit.etl_runs` (
            pipeline_name,
            run_timestamp,
            records_processed
        ) VALUES (
            'daily_sales_etl',
            CURRENT_TIMESTAMP(),
            (SELECT COUNT(*) FROM `my-project.staging.daily_sales`)
        );
        
        -- Limpiar tablas temporales
        DROP TABLE IF EXISTS `my-project.staging.temp_processing`;
    """
)

origin.add_output_pipe(pipe).set_destination(printer)
origin.pump()
```

---

### Ejemplo 8: Queries Parametrizadas
```python
from google.cloud import bigquery

# Query segura con par√°metros
origin = GCPBigQueryOrigin(
    name="filtered_sales",
    project_id="my-project",
    query="""
        SELECT * FROM dataset.sales 
        WHERE date >= @start_date 
        AND amount > @min_amount
    """,
    query_parameters=[
        bigquery.ScalarQueryParameter("start_date", "DATE", "2024-01-01"),
        bigquery.ScalarQueryParameter("min_amount", "FLOAT64", 100.0)
    ]
)

origin.add_output_pipe(pipe).set_destination(printer)
origin.pump()
```

---

### Ejemplo 9: Con Credenciales y Location
```python
# Usar service account y regi√≥n espec√≠fica
origin = GCPBigQueryOrigin(
    name="secure_extract",
    project_id="my-project",
    table="dataset.sensitive_data",
    credentials_path="/path/to/service-account.json",
    location="US",  # Regi√≥n de BigQuery
    timeout=300,    # Timeout de 5 minutos
    job_labels={
        "team": "data-engineering",
        "env": "production"
    }
)

origin.add_output_pipe(pipe).set_destination(printer)
origin.pump()
```

---

## üìä Output Ejemplo
```
GCPBigQueryOrigin 'daily_sales_etl' using default credentials
GCPBigQueryOrigin 'daily_sales_etl' BigQuery client initialized successfully

GCPBigQueryOrigin 'daily_sales_etl' executing before_query...
  Query preview: CREATE OR REPLACE TABLE `my-project.staging.daily_sales` AS...
‚úÖ GCPBigQueryOrigin 'daily_sales_etl' before_query executed successfully
  - Bytes processed: 1,234,567 bytes (1.18 MB)
  - Rows affected: 5,432
  - Duration: 2.34s

======================================================================
GCPBigQueryOrigin 'daily_sales_etl' executing MAIN extraction query...
======================================================================
  - Project ID: my-project
  - Query: SELECT * FROM `my-project.staging.daily_sales`

GCPBigQueryOrigin 'daily_sales_etl' waiting for query completion...

======================================================================
GCPBigQueryOrigin 'daily_sales_etl' MAIN query results:
======================================================================
  üìä Results:
     - Rows returned: 5,432
     - Columns: 6
     - Column names: ['sale_date', 'product_id', 'customer_id', 'amount', ...]
  
  üíæ Data processed:
     - Bytes processed: 234,567 bytes (0.22 GB)
     - Bytes billed: 234,567 bytes (0.22 GB)
  
  üí∞ Estimated cost: $0.001380 USD
  
  ‚è±Ô∏è  Job info:
     - Job ID: job_xyz123abc
     - Duration: 1.45s

GCPBigQueryOrigin 'daily_sales_etl' executing after_query...
  Query preview: INSERT INTO `my-project.audit.etl_runs` (...
‚úÖ GCPBigQueryOrigin 'daily_sales_etl' after_query executed successfully
  - Bytes processed: 12,345 bytes (0.01 MB)
  - Rows affected: 1
  - Duration: 0.89s

======================================================================
‚úÖ GCPBigQueryOrigin 'daily_sales_etl' pumped data through pipe 'pipe1'
======================================================================
```

---

## üìã Par√°metros Completos

| Par√°metro | Tipo | Requerido | Default | Descripci√≥n |
|-----------|------|-----------|---------|-------------|
| `name` | str | ‚úÖ | - | Nombre del componente |
| `project_id` | str | ‚úÖ | - | ID del proyecto GCP |
| `query` | str | * | None | Query SQL a ejecutar |
| `table` | str | * | None | Tabla en formato `dataset.table` |
| `credentials_path` | str | ‚ùå | None | Ruta al JSON de service account |
| `before_query` | str | ‚ùå | None | Query a ejecutar ANTES |
| `after_query` | str | ‚ùå | None | Query a ejecutar DESPU√âS |
| `max_results` | int | ‚ùå | None | L√≠mite de filas a retornar |
| `use_legacy_sql` | bool | ‚ùå | False | Usar SQL legacy |
| `query_parameters` | list | ‚ùå | [] | Par√°metros para queries |
| `location` | str | ‚ùå | None | Regi√≥n de BigQuery |
| `job_labels` | dict | ‚ùå | {} | Labels del job |
| `timeout` | float | ‚ùå | None | Timeout en segundos |
| `use_query_cache` | bool | ‚ùå | True | Usar cache de BigQuery |
| `dry_run` | bool | ‚ùå | False | Solo validar sin ejecutar |

\* **Nota**: Debes proporcionar `query` O `table`, pero no ambos.

---

## ‚úÖ Buenas Pr√°cticas

1. **Usa `dry_run`** antes de ejecutar queries grandes para estimar costos
2. **Usa `max_results`** en desarrollo para pruebas r√°pidas
3. **Usa `before_query`** para preparar datos y staging
4. **Usa `after_query`** para auditor√≠a y cleanup
5. **Usa `query_parameters`** en lugar de concatenar strings (seguridad)
6. **Usa `table`** cuando solo necesites `SELECT *` (m√°s simple)
7. **Especifica `location`** si trabajas con datos en regiones espec√≠ficas

---

## üîó Ver Tambi√©n

- [GCPBigQueryDestination](./GCPBigQueryDestination.md) - Para escribir a BigQuery
- [Open-Stage Documentation](../README.md) - Documentaci√≥n completa

---

**Open-Stage v2.3** - Enterprise ETL Framework